---
layout: default
---


# Audio samples of "FLOW-TTS: A NON-AUTOREGRESSIVE NETWORK FOR TEXT TO SPEECH BASED ON FLOW"

<!-- ## Authors

Chenfeng Miao*
Shuang Liang*
Minchuan Chen
Jun Ma
Shaojun Wang
Jing Xiao

<font size=2>\**Equal contribution.*</font> -->

## Abstract

In this work, we propose Flow-TTS, a non-autoregressive end-to-end neural TTS model based on generative flow. Unlike other non-autoregressive models, Flow-TTS can achieve high-quality speech generation by using a single feed-forward network. To our knowledge, Flow-TTS is the first TTS model utilizing flow in spectrogram generation network and the first non-autoregssive model which jointly learns the alignment and spectrogram generation through a single network. Experiments on LJSpeech show that the speech quality of Flow-TTS heavily approaches that of human and is even better than that of autoregressive model Tacotron 2 (outperforms Tacotron 2 with a gap of 0.09 in MOS). Meanwhile, the inference speed of Flow-TTS is about 23 times speed-up over Tacotron 2, which is comparable to FastSpeech.

### We compare our method with [Tacotron 2](https://arxiv.org/abs/1712.05884) (using pretrain model from [https://github.com/NVIDIA/tacotron2](https://github.com/NVIDIA/tacotron2)) and [FastSpeech](https://arxiv.org/abs/1905.09263) (using pretrain model from [https://github.com/espnet/espnet](https://github.com/espnet/espnet)). All of the audio samples use [WaveGlow](https://arxiv.org/abs/1811.00002v1) as vocoder.
